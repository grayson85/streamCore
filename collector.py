# collector.py
# StreamCore æ•°æ®é‡‡é›†æ¨¡å—
# æ”¯æŒ MacCMS V10 æ ‡å‡† JSON API æ ¼å¼

import time
import shutil
import os
import json
import sqlite3
import requests
import fcntl
from datetime import datetime
from db_config import get_db_connection, TEMP_DB, MAIN_DB, get_all_sources, init_db

# Lock file to prevent concurrent collector instances
LOCK_FILE = 'collector.lock'

def acquire_lock():
    """
    Acquire an exclusive lock to prevent concurrent collector runs.
    
    Returns:
        file object if lock acquired, None if another instance is running
    """
    try:
        lock_fd = open(LOCK_FILE, 'w')
        fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
        # Write PID and start time for debugging
        lock_fd.write(f"PID: {os.getpid()}\n")
        lock_fd.write(f"Started: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        lock_fd.flush()
        return lock_fd
    except (IOError, OSError):
        return None

def release_lock(lock_fd):
    """
    Release the lock and clean up lock file.
    
    Args:
        lock_fd: file object returned by acquire_lock()
    """
    if lock_fd:
        try:
            fcntl.flock(lock_fd, fcntl.LOCK_UN)
            lock_fd.close()
            if os.path.exists(LOCK_FILE):
                os.remove(LOCK_FILE)
        except:
            pass

def convert_time_to_timestamp(time_value):
    """
    å°†æ—¶é—´å€¼è½¬æ¢ä¸ºæ—¶é—´æˆ³
    
    Args:
        time_value: æ—¶é—´å€¼ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€æ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰
        
    Returns:
        int: Unix æ—¶é—´æˆ³
    """
    if isinstance(time_value, (int, float)):
        return int(time_value)
    
    if isinstance(time_value, str):
        # å°è¯•å¤šç§æ—¥æœŸæ—¶é—´æ ¼å¼
        formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
            '%Y-%m-%d',
            '%Y/%m/%d %H:%M:%S',
            '%Y/%m/%d',
        ]
        
        for fmt in formats:
            try:
                dt = datetime.strptime(time_value, fmt)
                return int(dt.timestamp())
            except ValueError:
                continue
        
        # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œå°è¯•ç›´æ¥è½¬æ¢ä¸ºæ•´æ•°
        try:
            return int(time_value)
        except ValueError:
            pass
    
    # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›å½“å‰æ—¶é—´
    return int(time.time())

def fetch_detail_data_from_source(source, vod_ids):
    """
    ä»èµ„æºç«™è·å–è¯¦ç»†æ•°æ®ï¼ˆåŒ…å«æ’­æ”¾åœ°å€ç­‰ï¼‰
    
    Args:
        source: èµ„æºç«™é…ç½®ï¼ˆdict-like å¯¹è±¡ï¼‰
        vod_ids: å½±ç‰‡IDåˆ—è¡¨ï¼ˆè¿œç¨‹IDï¼Œä¸å«å‰ç¼€ï¼‰
        
    Returns:
        dict: API å“åº”æ•°æ®ï¼Œå¤±è´¥è¿”å› None
    """
    if not vod_ids:
        return None
    
    # é‡è¯•é…ç½®
    max_retries = 3
    base_delay = 2
    
    for attempt in range(max_retries):
        try:
            # æ„é€  MacCMS æ ‡å‡† detail API URL
            url = source['res_url']
            separator = '&' if '?' in url else '?'
            
            # æ‹¼æ¥å¤šä¸ªIDï¼ˆé€—å·åˆ†éš”ï¼‰
            ids_str = ','.join(str(vid) for vid in vod_ids)
            full_url = f"{url}{separator}ac=detail&ids={ids_str}"
            
            # å‘é€ HTTP GET è¯·æ±‚
            response = requests.get(full_url, timeout=30)
            response.raise_for_status()
            
            # è§£æ JSON
            if source['data_format'] == 'json':
                try:
                    data = response.json()
                    return data
                except json.JSONDecodeError as e:
                    print(f"   âš ï¸  JSON è§£æå¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                    # ä»…åœ¨æœ€åä¸€æ¬¡å°è¯•å¤±è´¥æ—¶æ‰“å°å“åº”å†…å®¹
                    if attempt == max_retries - 1:
                        print(f"   ğŸ” å“åº”å†…å®¹ (å‰200å­—ç¬¦): {response.text[:200]!r}")
            else:
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"   âš ï¸  è¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
        except Exception as e:
            print(f"   âš ï¸  æœªçŸ¥é”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {e}")
            
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™ç­‰å¾…åé‡è¯•
        if attempt < max_retries - 1:
            wait_time = base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 2s, 4s...
            print(f"   â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
            
    return None

def update_vod_with_detail(temp_conn, source, detail_list):
    """
    ä½¿ç”¨è¯¦ç»†æ•°æ®æ›´æ–° sc_vod è¡¨
    
    Args:
        temp_conn: ä¸´æ—¶æ•°æ®åº“è¿æ¥
        source: èµ„æºç«™é…ç½®
        detail_list: è¯¦ç»†æ•°æ®åˆ—è¡¨ï¼ˆMacCMS æ ¼å¼ï¼‰
        
    Returns:
        int: æˆåŠŸæ›´æ–°çš„æ•°æ®æ¡æ•°
    """
    cursor = temp_conn.cursor()
    update_count = 0
    
    for vod in detail_list:
        try:
            # æå–è¿œç¨‹ID
            remote_vod_id = str(vod.get('vod_id', ''))
            if not remote_vod_id:
                continue
            
            # ç”Ÿæˆå…¨å±€å”¯ä¸€ ID
            res_unique_id = f"{source['res_id_prefix']}{remote_vod_id}"
            
            # Extract detail fields
            vod_en = vod.get('vod_en', '')
            vod_pic = vod.get('vod_pic', '')
            vod_play_url = vod.get('vod_play_url', '')
            vod_actor = vod.get('vod_actor', '')
            vod_director = vod.get('vod_director', '')
            vod_blurb = vod.get('vod_blurb', vod.get('vod_content', ''))
            vod_year = vod.get('vod_year', '')
            vod_area = vod.get('vod_area', '')
            vod_lang = vod.get('vod_lang', '')
            vod_class = vod.get('vod_class', '')
            vod_time_hits = int(vod.get('vod_time_hits', 0))
            
            # Update sc_vod table
            cursor.execute("""
                UPDATE sc_vod
                SET vod_en = ?,
                    vod_pic = ?,
                    vod_play_url = ?,
                    vod_actor = ?,
                    vod_director = ?,
                    vod_blurb = ?,
                    vod_year = ?,
                    vod_area = ?,
                    vod_lang = ?,
                    vod_class = ?,
                    vod_time_hits = ?
                WHERE res_unique_id = ?
            """, (vod_en, vod_pic, vod_play_url, vod_actor, vod_director, vod_blurb, vod_year, vod_area, vod_lang, vod_class, vod_time_hits, res_unique_id))
            
            if cursor.rowcount > 0:
                # æ›´æ–°æœç´¢è¡¨ï¼ˆæ·»åŠ æ¼”å‘˜ä¿¡æ¯ï¼‰
                cursor.execute("SELECT vod_id FROM sc_vod WHERE res_unique_id = ?", (res_unique_id,))
                result = cursor.fetchone()
                if result:
                    vod_id = result[0]
                    vod_name = vod.get('vod_name', '')
                    search_text = f"{vod_name} {vod_actor}".strip()
                    cursor.execute("""
                        UPDATE sc_search
                        SET search_text = ?
                        WHERE vod_id = ?
                    """, (search_text, vod_id))
                
                update_count += 1
            
        except Exception as e:
            print(f"   âš ï¸  æ›´æ–°è¯¦ç»†æ•°æ®å¤±è´¥: {e}")
            continue
    
    temp_conn.commit()
    return update_count

def collect_details_for_source(temp_conn, source):
    """
    ä¸ºèµ„æºç«™çš„æ‰€æœ‰å½±ç‰‡é‡‡é›†è¯¦ç»†æ•°æ®
    
    Args:
        temp_conn: ä¸´æ—¶æ•°æ®åº“è¿æ¥
        source: èµ„æºç«™é…ç½®
        
    Returns:
        int: æˆåŠŸæ›´æ–°çš„æ•°æ®æ¡æ•°
    """
    cursor = temp_conn.cursor()
    
    # æŸ¥è¯¢è¯¥èµ„æºç«™çš„æ‰€æœ‰éœ€è¦æ›´æ–°çš„å½±ç‰‡ID (vod_play_url ä¸ºç©º)
    cursor.execute("""
        SELECT res_unique_id FROM sc_vod
        WHERE res_source_id = ? AND (vod_play_url IS NULL OR vod_play_url = '')
    """, (source['res_source_id'],))
    
    all_unique_ids = [row[0] for row in cursor.fetchall()]
    
    if not all_unique_ids:
        print(f"   âœ… æ‰€æœ‰æ•°æ®å·²å®Œæˆè¯¦æƒ…åŒæ­¥ï¼Œæ— éœ€æ›´æ–°")
        return 0
    
    # æå–è¿œç¨‹IDï¼ˆå»æ‰å‰ç¼€ï¼‰
    prefix_len = len(source['res_id_prefix'])
    remote_ids = [uid[prefix_len:] for uid in all_unique_ids]
    
    total_count = len(remote_ids)
    print(f"\n   ğŸ“¸ å¼€å§‹é‡‡é›†è¯¦ç»†æ•°æ®ï¼ˆæ’­æ”¾åœ°å€ã€æµ·æŠ¥ç­‰ï¼‰...")
    print(f"   ğŸ“Š å¾…å¤„ç†: {total_count} æ¡")
    
    # æ‰¹é‡å¤„ç†ï¼ˆæ¯æ‰¹20ä¸ªï¼‰ - API é™åˆ¶å•æ¬¡æœ€å¤§è¿”å› 20 æ¡
    batch_size = 20
    update_count = 0
    
    for i in range(0, total_count, batch_size):
        batch = remote_ids[i:i+batch_size]
        batch_num = (i // batch_size) + 1
        total_batches = (total_count + batch_size - 1) // batch_size
        
        print(f"   ğŸ”„ æ‰¹æ¬¡ {batch_num}/{total_batches} (å¤„ç† {len(batch)} æ¡)...", end=' ')
        
        # è·å–è¯¦ç»†æ•°æ®
        detail_data = fetch_detail_data_from_source(source, batch)
        
        if detail_data and detail_data.get('code') == 1:
            detail_list = detail_data.get('list', [])
            count = update_vod_with_detail(temp_conn, source, detail_list)
            update_count += count
            print(f"âœ… æ›´æ–° {count} æ¡")
        else:
            print(f"âŒ å¤±è´¥")
    
    return update_count

def sync_hot_rank_to_new_records(conn):
    """
    Synchronize hot_rank to the newest record for each video.
    Specific to handling cases where a new ID is generated for the same video (e.g. new episodes).
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ”„ Syncing Hot Rank to Newest Records")
    print(f"=" * 70)
    
    cursor = conn.cursor()
    
    try:
        # Check if hot_rank column exists
        cursor.execute("PRAGMA table_info(sc_vod)")
        columns = [column[1] for column in cursor.fetchall()]
        if 'hot_rank' not in columns:
            print("   âš ï¸  hot_rank column not found, skipping sync")
            return 0

        # Find all names that have a hot_rank set to something other than 0
        cursor.execute("SELECT DISTINCT vod_name FROM sc_vod WHERE hot_rank > 0")
        hot_names = [row[0] for row in cursor.fetchall()]
        
        migrated_count = 0
        
        for name in hot_names:
            try:
                # Find all records for this name, ordered by vod_time DESC, then vod_id DESC
                cursor.execute("SELECT vod_id, hot_rank, vod_time FROM sc_vod WHERE vod_name = ? ORDER BY vod_time DESC, vod_id DESC", (name,))
                records = cursor.fetchall()
                
                if not records or len(records) < 1:
                    continue
                    
                # Newest is the first record
                newest_id = records[0][0]
                
                # Calculate the max rank among all these records (to preserve the highest rank if duplicates have different ranks)
                target_rank = 0
                for r in records:
                    if r[1] > 0:
                        target_rank = max(target_rank, r[1])
                
                # Check if we need to update anything
                # We need update if:
                # 1. The newest record doesn't have the target rank
                # 2. OR any other older record HAS a non-zero rank
                
                needs_update = False
                if records[0][1] != target_rank:
                    needs_update = True
                
                for r in records[1:]:
                    if r[1] > 0:
                        needs_update = True
                        break
                
                if not needs_update:
                    continue

                print(f"   ğŸ”„ Migrating hot_rank {target_rank} for '{name}' to newest ID {newest_id}")
                
                # Reset all to 0 first
                cursor.execute("UPDATE sc_vod SET hot_rank = 0 WHERE vod_name = ?", (name,))
                
                # Set the newest one to target_rank
                cursor.execute("UPDATE sc_vod SET hot_rank = ? WHERE vod_id = ?", (target_rank, newest_id))
                
                migrated_count += 1
                
            except Exception as e:
                print(f"   âš ï¸  Error syncing '{name}': {e}")
                
        conn.commit()
        if migrated_count > 0:
            print(f"âœ… Migrated hot_rank for {migrated_count} videos")
        else:
            print(f"âœ… No queries needed migration")
            
        return migrated_count
            
    except Exception as e:
        print(f"âŒ Hot rank sync failed: {e}")
        return 0

def swap_database_files():
    """
    åŸå­æ€§æ›¿æ¢æ•°æ®åº“æ–‡ä»¶ï¼Œå°† TEMP_DB è¦†ç›– MAIN_DB
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    if not os.path.exists(TEMP_DB):
        print(f"âŒ ä¸´æ—¶æ•°æ®åº“æ–‡ä»¶ {TEMP_DB} ä¸å­˜åœ¨ï¼Œé‡‡é›†å¤±è´¥æˆ–æœªå®Œæˆã€‚")
        return False
    
    try:
        # å¤‡ä»½æ—§ä¸»æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(MAIN_DB):
            backup_name = f"{MAIN_DB}.{time.strftime('%Y%m%d%H%M%S')}.bak"
            shutil.copy2(MAIN_DB, backup_name)
            print(f"ğŸ“¦ å·²å¤‡ä»½æ—§æ•°æ®åº“ï¼š{backup_name}")
        
        # åŸå­æ€§æ›¿æ¢
        shutil.move(TEMP_DB, MAIN_DB)
        print(f"âœ… æ–‡ä»¶åˆ‡æ¢æˆåŠŸï¼š{TEMP_DB} â†’ {MAIN_DB}")
        
        # ğŸ’¾ ä¸å†åˆ›å»º sc_temp.dbï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
        # ä¸‹æ¬¡é‡‡é›†æ—¶ä¼šè‡ªåŠ¨ä» sc_main.db å¤åˆ¶
        print(f"ğŸ’¾ å·²æ¸…ç†ä¸´æ—¶æ•°æ®åº“ï¼ŒèŠ‚çœ {os.path.getsize(MAIN_DB) // 1024 // 1024}MB ç©ºé—´")
        
        # æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘1ä¸ª
        cleanup_old_backups(MAIN_DB, keep_count=1)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶åˆ‡æ¢å¼‚å¸¸: {e}")
        return False

def cleanup_old_backups(db_name, keep_count=3):
    """
    æ¸…ç†æ—§çš„æ•°æ®åº“å¤‡ä»½æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘çš„ N ä¸ª
    
    Args:
        db_name: ä¸»æ•°æ®åº“æ–‡ä»¶å
        keep_count: ä¿ç•™çš„å¤‡ä»½æ•°é‡
    """
    try:
        # è·å–å½“å‰ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(db_name)) or '.'
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤‡ä»½æ–‡ä»¶ï¼ˆæ ¼å¼ï¼šsc_main.db.YYYYMMDDHHMMSS.bakï¼‰
        import glob
        backup_pattern = f"{db_name}.*.bak"
        backup_files = glob.glob(os.path.join(current_dir, backup_pattern))
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        backup_files.sort(key=os.path.getmtime, reverse=True)
        
        # åˆ é™¤è¶…å‡ºä¿ç•™æ•°é‡çš„å¤‡ä»½
        if len(backup_files) > keep_count:
            files_to_delete = backup_files[keep_count:]
            for old_backup in files_to_delete:
                try:
                    os.remove(old_backup)
                    print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ—§å¤‡ä»½ï¼š{os.path.basename(old_backup)}")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤å¤‡ä»½å¤±è´¥ {os.path.basename(old_backup)}: {e}")
            
            print(f"âœ… å¤‡ä»½æ¸…ç†å®Œæˆï¼Œä¿ç•™æœ€è¿‘ {keep_count} ä¸ªå¤‡ä»½")
    
    except Exception as e:
        print(f"âš ï¸  å¤‡ä»½æ¸…ç†å¼‚å¸¸: {e}")


def fetch_data_from_source(source, page=1, hours=None):
    """
    ä»èµ„æºç«™è·å–æ•°æ®
    
    Args:
        source: èµ„æºç«™é…ç½®ï¼ˆdict-like å¯¹è±¡ï¼‰
        page: é¡µç 
        hours: å¯é€‰ï¼Œå¢é‡é‡‡é›†æ—¶é—´çª—å£ï¼ˆå°æ—¶æ•°ï¼‰ï¼ŒNone è¡¨ç¤ºå…¨é‡é‡‡é›†
        
    Returns:
        dict: API å“åº”æ•°æ®ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        # æ„é€  MacCMS æ ‡å‡† API URL
        url = source['res_url']
        
        # æ·»åŠ åˆ†é¡µå‚æ•°
        separator = '&' if '?' in url else '?'
        full_url = f"{url}{separator}ac=list&pg={page}"
        
        # å¦‚æœæ˜¯å¢é‡æ¨¡å¼ï¼Œæ·»åŠ æ—¶é—´è¿‡æ»¤å‚æ•°
        if hours is not None:
            full_url += f"&h={hours}"
        
        print(f"   ğŸŒ è¯·æ±‚: {full_url}")
        
        try:
            resp = requests.get(full_url, timeout=30)
            resp.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"   âŒ è¯·æ±‚å¤±è´¥: {e}")
            print(f"   âš ï¸  è·³è¿‡ç¬¬ {page} é¡µï¼Œç»§ç»­ä¸‹ä¸€é¡µ...")
            # This function is not in a loop that can 'continue'.
            # It should return None to indicate failure.
            return None 
        
        try:
            data = resp.json()
            return data
        except ValueError as e:
            print(f"   âŒ JSON è§£æå¤±è´¥: {e}")
            print(f"   âš ï¸  è·³è¿‡ç¬¬ {page} é¡µï¼Œç»§ç»­ä¸‹ä¸€é¡µ...")
            # This function is not in a loop that can 'continue'.
            # It should return None to indicate failure.
            return None
            
    except requests.exceptions.Timeout:
        print(f"   âŒ è¯·æ±‚è¶…æ—¶")
        return None
    except requests.exceptions.RequestException as e:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"   âŒ JSON è§£æå¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"   âŒ æœªçŸ¥é”™è¯¯: {e}")
        return None

def parse_and_insert_vod_data(temp_conn, source, vod_list):
    """
    è§£æå¹¶æ’å…¥å½±ç‰‡æ•°æ®åˆ°ä¸´æ—¶æ•°æ®åº“
    
    Args:
        temp_conn: ä¸´æ—¶æ•°æ®åº“è¿æ¥
        source: èµ„æºç«™é…ç½®
        vod_list: å½±ç‰‡åˆ—è¡¨ï¼ˆMacCMS æ ¼å¼ï¼‰
        
    Returns:
        int: æˆåŠŸæ’å…¥çš„æ•°æ®æ¡æ•°
    """
    cursor = temp_conn.cursor()
    insert_count = 0
    skip_count = 0
    
    # åŠ è½½åˆ†ç±»æ˜ å°„
    mapping = {}
    if source['res_mapping']:
        try:
            mapping = json.loads(source['res_mapping'])
        except:
            print(f"   âš ï¸  åˆ†ç±»æ˜ å°„è§£æå¤±è´¥")
    
    # æ£€æŸ¥æ˜ å°„æ˜¯å¦ä¸ºç©º
    if not mapping:
        print(f"   âš ï¸  è­¦å‘Šï¼šèµ„æºç«™ '{source['res_name']}' æ²¡æœ‰é…ç½®åˆ†ç±»æ˜ å°„ï¼")
        print(f"   ğŸ’¡ è¯·å…ˆä½¿ç”¨ 'python setup.py map-source-types --source {source['res_id_prefix']}' é…ç½®æ˜ å°„")
        print(f"   â­ï¸  è·³è¿‡æ­¤èµ„æºç«™çš„æ•°æ®é‡‡é›†")
        return 0
    
    for vod in vod_list:
        try:
            # æå– MacCMS æ ‡å‡†å­—æ®µ
            remote_vod_id = str(vod.get('vod_id', ''))
            vod_name = vod.get('vod_name', '').strip()
            remote_type_id = str(vod.get('type_id', '0'))
            
            if not remote_vod_id or not vod_name:
                continue
            
            # ç”Ÿæˆå…¨å±€å”¯ä¸€ ID
            res_unique_id = f"{source['res_id_prefix']}{remote_vod_id}"
            
            # ğŸ¯ ä¸¥æ ¼æ˜ å°„æ¨¡å¼ï¼šå¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œè·³è¿‡æ•°æ®
            vod_type_id = mapping.get(remote_type_id)
            
            if vod_type_id is None:
                # è·³è¿‡æ— æ˜ å°„çš„æ•°æ®ï¼Œé¿å…è„æ•°æ®
                skip_count += 1
                if skip_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªè­¦å‘Šï¼Œé¿å…åˆ·å±
                    print(f"   âš ï¸  è·³è¿‡æ— æ˜ å°„æ•°æ®: {vod_name} (è¿œç¨‹åˆ†ç±»ID: {remote_type_id})")
                continue
            
            # Other fields
            vod_en = vod.get('vod_en', '')
            vod_pic = vod.get('vod_pic', '')
            vod_remarks = vod.get('vod_remarks', '')
            vod_blurb = vod.get('vod_blurb', vod.get('vod_content', ''))  # ç®€ä»‹
            vod_actor = vod.get('vod_actor', '')
            vod_director = vod.get('vod_director', '')
            vod_play_url = vod.get('vod_play_url', '')
            vod_play_from = vod.get('vod_play_from', '')
            
            # Normalize source names: wjm3u8 -> wj, mtm3u8 -> mt
            if vod_play_from:
                vod_play_from = vod_play_from.replace('wjm3u8', 'wj').replace('mtm3u8', 'mt')
            
            vod_year = vod.get('vod_year', '')
            vod_area = vod.get('vod_area', '')
            vod_lang = vod.get('vod_lang', '')
            vod_class = vod.get('vod_class', '')
            
            # æ—¶é—´å¤„ç†ï¼šæ”¯æŒå­—ç¬¦ä¸²å’Œæ—¶é—´æˆ³ä¸¤ç§æ ¼å¼
            vod_time_raw = vod.get('vod_time', None)
            if vod_time_raw is not None:
                vod_time = convert_time_to_timestamp(vod_time_raw)
            else:
                vod_time = int(time.time())
            
            vod_time_hits = int(vod.get('vod_time_hits', 0))
            vod_hits = int(vod.get('vod_hits', 0))
            
            # æ’å…¥ sc_vod è¡¨ï¼ˆä½¿ç”¨ INSERT OR REPLACE å»é‡ï¼‰
            cursor.execute("""
                INSERT OR REPLACE INTO sc_vod (
                    res_unique_id, res_source_id, vod_name, vod_type_id,
                    vod_en, vod_pic, vod_remarks, vod_blurb, vod_play_url, vod_play_from,
                    vod_actor, vod_director, vod_year, vod_area, vod_lang, vod_class,
                    vod_time, vod_time_hits, vod_hits
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                res_unique_id, source['res_source_id'], vod_name, vod_type_id,
                vod_en, vod_pic, vod_remarks, vod_blurb, vod_play_url, vod_play_from,
                vod_actor, vod_director, vod_year, vod_area, vod_lang, vod_class,
                vod_time, vod_time_hits, vod_hits
            ))
            
            # è·å–æ’å…¥åçš„ vod_id
            cursor.execute("SELECT vod_id FROM sc_vod WHERE res_unique_id = ?", (res_unique_id,))
            result = cursor.fetchone()
            if result:
                vod_id = result[0]
                
                # æ›´æ–° sc_search è¡¨ï¼ˆç”¨äºæœç´¢ä¼˜åŒ–ï¼‰
                search_text = f"{vod_name} {vod_actor}".strip()
                cursor.execute("""
                    INSERT OR REPLACE INTO sc_search (vod_id, search_text)
                    VALUES (?, ?)
                """, (vod_id, search_text))
                
                insert_count += 1
            
        except Exception as e:
            print(f"   âš ï¸  å¤„ç†æ•°æ®å¤±è´¥: {e} - æ•°æ®: {vod.get('vod_name', 'N/A')}")
            continue
    
    temp_conn.commit()
    
    # æŠ¥å‘Šè·³è¿‡çš„æ•°æ®
    if skip_count > 0:
        print(f"   â„¹ï¸  å…±è·³è¿‡ {skip_count} æ¡æ— æ˜ å°„æ•°æ®")
    
    return insert_count

def collect_from_source(temp_conn, source, hours=None, auto_confirm=False):
    """
    ä»å•ä¸ªèµ„æºç«™é‡‡é›†æ‰€æœ‰æ•°æ®
    
    Args:
        temp_conn: ä¸´æ—¶æ•°æ®åº“è¿æ¥
        source: èµ„æºç«™é…ç½®
        hours: å¯é€‰ï¼Œå¢é‡é‡‡é›†æ—¶é—´çª—å£ï¼ˆå°æ—¶æ•°ï¼‰ï¼ŒNone è¡¨ç¤ºå…¨é‡é‡‡é›†
        auto_confirm: éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨ç¡®è®¤é‡‡é›†ï¼ˆç”¨äº cron/è‡ªåŠ¨åŒ–ï¼‰
        
    Returns:
        int: æ€»å…±é‡‡é›†çš„æ•°æ®æ¡æ•°
    """
    print(f"\nğŸ“¡ é‡‡é›†èµ„æºç«™: {source['res_name']} ({source['res_id_prefix']})")
    print(f"   URL: {source['res_url']}")
    
    # å…ˆè·å–ç¬¬ä¸€é¡µï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ” æ­£åœ¨è·å–èµ„æºç«™ä¿¡æ¯...")
    first_page_data = fetch_data_from_source(source, 1, hours)
    
    if not first_page_data or first_page_data.get('code') != 1:
        print(f"   âŒ æ— æ³•è·å–èµ„æºç«™ä¿¡æ¯")
        return 0
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    total_pages = first_page_data.get('pagecount', 1)
    total_records = first_page_data.get('total', 0)
    per_page = first_page_data.get('limit', 20)
    
    print(f"\nğŸ“Š èµ„æºç«™ç»Ÿè®¡")
    print("=" * 50)
    print(f"   æ€»é¡µæ•°: {total_pages:,} é¡µ")
    print(f"   æ€»è®°å½•: {total_records:,} æ¡")
    print(f"   æ¯é¡µæ•°: {per_page} æ¡")
    
    # Initialize pagination variables
    start_page = 1
    max_pages = total_pages
    
    # å¢é‡æ¨¡å¼ï¼šè·³è¿‡ç”¨æˆ·ç¡®è®¤ï¼Œç›´æ¥é‡‡é›†æ‰€æœ‰ç»“æœ
    if hours is not None:
        print(f"   ğŸ“Š å¢é‡é‡‡é›†ï¼šæœ€è¿‘ {hours} å°æ—¶å†…æ›´æ–°çš„æ•°æ®")
        print("=" * 50)
    else:
        # å…¨é‡æ¨¡å¼ï¼šæ˜¾ç¤ºé¢„ä¼°æ—¶é—´å¹¶è¯¢é—®ç”¨æˆ·
        estimated_minutes = (total_pages * 0.5) / 60
        print(f"   é¢„è®¡è€—æ—¶: {estimated_minutes:.1f} åˆ†é’Ÿ")
        print("=" * 50)
        
        # è¯¢é—®ç”¨æˆ·ç¡®è®¤ (é™¤é --yes æ¨¡å¼)
        if auto_confirm:
            print(f"\nâœ… éäº¤äº’æ¨¡å¼ (--yes)ï¼šè‡ªåŠ¨é‡‡é›†å…¨éƒ¨ {total_pages:,} é¡µ")
        else:
            print(f"\nâš ï¸  é‡‡é›†é€‰é¡¹ï¼š")
            print(f"   [1] é‡‡é›†å…¨éƒ¨ ({total_pages:,} é¡µ)")
            print(f"   [2] é‡‡é›†éƒ¨åˆ†ï¼ˆæŒ‡å®šé¡µç èŒƒå›´ï¼‰")
            print(f"   [3] å–æ¶ˆé‡‡é›†")
            
            choice = input("\nâ†’ è¯·é€‰æ‹© [1/2/3ï¼Œé»˜è®¤:1]: ").strip() or '1'
            
            if choice == '3':
                print("\nâŒ å·²å–æ¶ˆé‡‡é›†")
                return 0
            
            if choice == '2':
                try:
                    print(f"\nâ†’ è¯·è¾“å…¥é¡µç èŒƒå›´ï¼š")
                    page_range = input(f"   æ ¼å¼: èµ·å§‹é¡µ-ç»“æŸé¡µ (ä¾‹å¦‚: 1-10 æˆ– 5-15) [1-{total_pages}]: ").strip()
                    
                    if '-' in page_range:
                        # Parse range like "5-10"
                        start_str, end_str = page_range.split('-', 1)
                        start_page = int(start_str.strip())
                        end_page = int(end_str.strip())
                        
                        # Validate range
                        if start_page < 1:
                            start_page = 1
                        if end_page > total_pages:
                            end_page = total_pages
                        if start_page > end_page:
                            start_page, end_page = end_page, start_page  # Swap if reversed
                        
                        max_pages = end_page
                        print(f"   âœ… å°†é‡‡é›†ç¬¬ {start_page} åˆ° {end_page} é¡µ (å…± {end_page - start_page + 1} é¡µ)")
                    else:
                        # Just a number, treat as number of pages from start
                        num_pages = int(page_range)
                        if 1 <= num_pages <= total_pages:
                            max_pages = num_pages
                            print(f"   âœ… å°†é‡‡é›†å‰ {num_pages} é¡µ")
                        else:
                            print(f"   âš ï¸  é¡µæ•°è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨å…¨éƒ¨é¡µæ•°: {total_pages}")
                            
                except (ValueError, AttributeError):
                    print(f"   âš ï¸  è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨å…¨éƒ¨é¡µæ•°: {total_pages}")
                    start_page = 1
                    max_pages = total_pages
    
    # å¼€å§‹é‡‡é›†
    print(f"\nğŸš€ å¼€å§‹é‡‡é›†...")
    total_count = 0
    page = start_page  # Start from user-specified page
    
    while True:
        print(f"\n   ğŸ“„ é‡‡é›†ç¬¬ {page}/{max_pages} é¡µ...")
        
        # ç¬¬ä¸€é¡µå·²ç»è·å–è¿‡ï¼Œç›´æ¥ä½¿ç”¨
        if page == 1 and start_page == 1:
            data = first_page_data
        else:
            # è·å–æ•°æ®
            data = fetch_data_from_source(source, page, hours)
            
            if not data:
                print(f"   âš ï¸  ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡å¹¶ç»§ç»­...")
                page += 1
                continue  # Skip failed page - exits this iteration completely
            
            # æ£€æŸ¥å“åº”ç 
            if data.get('code') != 1:
                print(f"   âš ï¸  API è¿”å›é”™è¯¯: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}ï¼Œè·³è¿‡...")
                page += 1
                continue  # Skip page with API error

        
        # æå–å½±ç‰‡åˆ—è¡¨
        vod_list = data.get('list', [])
        
        if not vod_list:
            print(f"   âœ… ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œé‡‡é›†å®Œæˆ")
            break
        
        # è§£æå¹¶æ’å…¥æ•°æ®
        count = parse_and_insert_vod_data(temp_conn, source, vod_list)
        total_count += count
        
        print(f"   âœ… ç¬¬ {page} é¡µå¤„ç†å®Œæˆï¼Œæ–°å¢/æ›´æ–° {count} æ¡æ•°æ®")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç”¨æˆ·è®¾å®šçš„é¡µæ•°é™åˆ¶
        if page >= max_pages:
            print(f"   âœ… å·²è¾¾åˆ°è®¾å®šé¡µæ•° ({max_pages})ï¼Œé‡‡é›†å®Œæˆ")
            break
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
        try:
            current_page = int(data.get('page', page))
            total_pages_api = int(data.get('pagecount', 1))
        except (ValueError, TypeError):
            current_page = page
            total_pages_api = 1
        
        if current_page >= total_pages_api:
            print(f"   âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ ({total_pages_api})ï¼Œé‡‡é›†å®Œæˆ")
            break
        
        page += 1
        
        # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…å¯¹èµ„æºç«™é€ æˆå‹åŠ›
        time.sleep(0.5)
    
    return total_count

def run_collection():
    """ä¸»é‡‡é›†å‡½æ•°"""
    import argparse
    
    # Create argument parser
    parser = argparse.ArgumentParser(
        description='StreamCore Data Collector - Collect video data from multiple sources',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Full collection (default)
  python3 collector.py --mode full
  
  # Incremental collection (last 6 hours)
  python3 collector.py --mode incremental --hours 6
  
  # Details only (skip list collection)
  python3 collector.py --mode details-only

Note: All modes automatically merge duplicates before database swap!
        '''
    )
    
    parser.add_argument(
        '--mode',
        choices=['full', 'incremental', 'details-only'],
        default='full',
        help='Collection mode (default: full)'
    )
    
    parser.add_argument(
        '--hours',
        type=int,
        metavar='N',
        help='Hours for incremental mode (e.g., --mode incremental --hours 6)'
    )
    
    parser.add_argument(
        '-y', '--yes',
        action='store_true',
        help='Non-interactive mode: auto-confirm all prompts (required for cron/automation)'
    )
    
    args = parser.parse_args()
    
    # Extract arguments
    mode = args.mode
    hours = args.hours
    auto_confirm = args.yes
    
    # Validate mode-specific requirements
    if mode == 'incremental' and hours is None:
        parser.error("--hours argument required for incremental mode")
    
    # Set details_only flag
    details_only = (mode == 'details-only')

    # ğŸ”’ Acquire lock to prevent concurrent runs
    lock_fd = acquire_lock()
    if lock_fd is None:
        print("âŒ å¦ä¸€ä¸ªé‡‡é›†å™¨å®ä¾‹æ­£åœ¨è¿è¡Œï¼")
        print("   å¦‚æœç¡®å®šæ²¡æœ‰å…¶ä»–å®ä¾‹ï¼Œè¯·åˆ é™¤ collector.lock æ–‡ä»¶åé‡è¯•")
        print("   rm collector.lock")
        return
    
    try:
        print("=" * 70)
        if details_only:
            mode_text = "ä»…é‡‡é›†è¯¦ç»†æ•°æ®æ¨¡å¼"
        elif mode == 'incremental':
            mode_text = f"å¢é‡æ¨¡å¼ - æœ€è¿‘ {hours} å°æ—¶"
        else:
            mode_text = "å…¨é‡æ¨¡å¼"
        
        print(f"ğŸš€ StreamCore é‡‡é›†ä»»åŠ¡å¯åŠ¨ ({mode_text})")
        print(f"ğŸ”’ è¿›ç¨‹é”å·²è·å– (PID: {os.getpid()})")
        print("=" * 70)
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
        # ğŸ” Check if temp DB exists BEFORE connecting (which would create an empty file)
        if not os.path.exists(TEMP_DB):
            if os.path.exists(MAIN_DB):
                print(f"ğŸ“¦ æ­£åœ¨ä»ä¸»æ•°æ®åº“å¤åˆ¶æ•°æ®åˆ°ä¸´æ—¶åº“...")
                shutil.copy2(MAIN_DB, TEMP_DB)
            else:
                print(f"ğŸ†• ä¸»æ•°æ®åº“ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–æ–°çš„ä¸´æ—¶æ•°æ®åº“...")
                init_db(TEMP_DB)
        
        # Now connect (file exists and is populated/initialized)
        temp_conn = get_db_connection(TEMP_DB)

        # ğŸ›¡ï¸ Self-healing: Check if the database is valid (has tables)
        # If the file exists but is empty (e.g. created by previous bug), table check will fail
        try:
            temp_conn.execute("SELECT 1 FROM sc_vod LIMIT 1")
        except sqlite3.OperationalError:
            print(f"âš ï¸  æ£€æµ‹åˆ°ä¸´æ—¶æ•°æ®åº“æŸåæˆ–æœªåˆå§‹åŒ– (æ—  sc_vod è¡¨)")
            print(f"ğŸ”„ æ­£åœ¨å°è¯•è‡ªåŠ¨ä¿®å¤...")
            temp_conn.close()
            
            if os.path.exists(MAIN_DB):
                print(f"ğŸ“¦ ä»ä¸»æ•°æ®åº“é‡æ–°å¤åˆ¶...")
                shutil.copy2(MAIN_DB, TEMP_DB)
            else:
                print(f"ğŸ†• é‡æ–°åˆå§‹åŒ–æ•°æ®åº“ç»“æ„...")
                init_db(TEMP_DB)
            
            # Reconnect
            temp_conn = get_db_connection(TEMP_DB)

        main_conn = get_db_connection(MAIN_DB)
        sources = get_all_sources(main_conn)
        main_conn.close()
        
        if not sources:
            print("\nâŒ æœªé…ç½®ä»»ä½•èµ„æºç«™ï¼Œè¯·å…ˆä½¿ç”¨ 'python setup.py add-source' æ·»åŠ èµ„æºç«™")
            temp_conn.close()
            return
        
        print(f"\nğŸ“‹ å…±æ‰¾åˆ° {len(sources)} ä¸ªèµ„æºç«™é…ç½®")
        
        total_collected = 0
        success_count = 0
        
        # 1. åˆ—è¡¨é‡‡é›†é˜¶æ®µ (é details-only æ¨¡å¼ä¸‹æ‰§è¡Œ)
        if not details_only:
            for source in sources:
                try:
                    count = collect_from_source(temp_conn, source, hours, auto_confirm)
                    total_collected += count
                    success_count += 1
                except Exception as e:
                    print(f"\nâŒ é‡‡é›†èµ„æºç«™ {source['res_name']} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    continue
        else:
            print("\nâ­ï¸  è·³è¿‡åˆ—è¡¨é‡‡é›†é˜¶æ®µ (details-only)")
            # ç»Ÿè®¡ä¸€ä¸‹æœ‰å¤šå°‘æ•°æ®éœ€è¦æ›´æ–°è¯¦æƒ…
            for source in sources:
                cursor = temp_conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM sc_vod WHERE res_source_id=?", (source['res_source_id'],))
                count = cursor.fetchone()[0]
                total_collected += count
                success_count += 1

        # 2. è¯¦ç»†æ•°æ®é‡‡é›†é˜¶æ®µ (è‡ªåŠ¨é‡‡é›†æ’­æ”¾åœ°å€ã€æµ·æŠ¥ç­‰)
        detail_total = 0  # Initialize here to prevent UnboundLocalError
        if total_collected > 0:
            print("\n" + "=" * 70)
            print("ğŸ“¸ é‡‡é›†è¯¦ç»†æ•°æ®ï¼ˆæ’­æ”¾åœ°å€ã€æµ·æŠ¥ã€æ¼”å‘˜ç­‰ï¼‰")
            print("=" * 70)
            
            for source in sources:
                try:
                    count = collect_details_for_source(temp_conn, source)
                    detail_total += count
                except Exception as e:
                    print(f"\nâŒ é‡‡é›†è¯¦ç»†æ•°æ®å¤±è´¥ {source['res_name']}: {e}")
                    continue
            
            print(f"\nâœ… è¯¦ç»†æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±æ›´æ–° {detail_total} æ¡è®°å½•")
        
        # ğŸ†• Sync hot_rank to newest records (Server-side fix for stale IDs)
        # Always run this to auto-heal the database even if no new data was collected
        migrated_hot_rank_count = sync_hot_rank_to_new_records(temp_conn)
        
        temp_conn.close()
        
        # Summary statistics
        print(f"\n======================================================================")
        print(f"ğŸ“Š é‡‡é›†ç»Ÿè®¡")
        print(f"======================================================================")
        print(f"âœ… å¤„ç†èµ„æºç«™æ•°: {success_count}/{len(sources)}")
        print(f"ğŸ“¦ æ–°å¢è®°å½•æ•°: {total_collected}")
        print(f"ğŸ“¦ æ›´æ–°è¯¦ç»†ä¿¡æ¯æ•°: {detail_total}")
        print(f"ğŸ”„ çƒ­æ¦œè¿ç§»æ•°: {migrated_hot_rank_count}")
        print(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ğŸ†• Auto-merge duplicates in TEMP database (before swap)
        if total_collected > 0 or detail_total > 0:
            print(f"\n======================================================================")
            print(f"ğŸ”„ Auto-Merging Duplicates")
            print(f"======================================================================")
            
            try:
                from merge_dedupe import auto_merge
                merge_stats = auto_merge(TEMP_DB)
                
                if merge_stats['groups_merged'] > 0:
                    print(f"âœ… Merged {merge_stats['groups_merged']} duplicate groups")
                    print(f"âœ… Deleted {merge_stats['records_deleted']} duplicate records")
                else:
                    print(f"   â„¹ï¸  No duplicates found to merge")
            except Exception as e:
                print(f"âš ï¸  Merge failed: {e}")
                print(f"   Continuing without merge...")
        
        # Execute database file switch
        # Enable swap if ANY data was changed (collected, detail updated, OR hot rank migrated)
        if total_collected > 0 or detail_total > 0 or migrated_hot_rank_count > 0:
            print("\n" + "=" * 70)
            print("ğŸ”„ æ‰§è¡Œæ•°æ®åº“æ–‡ä»¶åˆ‡æ¢")
            print("=" * 70)
            
            if swap_database_files():
                print("\nâœ… é‡‡é›†ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼æ–°æ•°æ®å·²ç”Ÿæ•ˆã€‚")
            else:
                print("\nâŒ æ–‡ä»¶åˆ‡æ¢å¤±è´¥ï¼ŒAPI æœåŠ¡å°†ç»§ç»­ä½¿ç”¨æ—§æ•°æ®")
        else:
            print("\nâš ï¸  æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®ï¼Œè·³è¿‡æ–‡ä»¶åˆ‡æ¢")
        
        print("=" * 70)
    
    finally:
        # ğŸ”“ Always release lock when done
        release_lock(lock_fd)
        print("ğŸ”“ è¿›ç¨‹é”å·²é‡Šæ”¾")

if __name__ == '__main__':
    run_collection()
