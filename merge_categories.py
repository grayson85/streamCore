#!/usr/bin/env python3
# merge_categories.py
# åˆå¹¶é‡å¤åˆ†ç±» - ç›¸åŒåç§°å’Œçˆ¶åˆ†ç±»çš„åˆ†ç±»ä¿ç•™æœ€å°ID

import sqlite3
import json
import shutil
import os
from datetime import datetime
from db_config import get_db_connection, MAIN_DB, TEMP_DB


def find_duplicate_categories(conn):
    """
    æŸ¥æ‰¾é‡å¤åˆ†ç±»ï¼ˆç›¸åŒåç§°+ç›¸åŒçˆ¶åˆ†ç±»ï¼‰
    
    Returns:
        list: [(keep_id, delete_id, name, parent_id), ...]
    """
    cursor = conn.cursor()
    
    # æŸ¥æ‰¾é‡å¤ï¼šç›¸åŒ type_name å’Œ type_pidï¼Œä¿ç•™æœ€å° type_id
    cursor.execute("""
        SELECT 
            MIN(type_id) as keep_id,
            GROUP_CONCAT(type_id) as all_ids,
            type_name,
            type_pid,
            COUNT(*) as cnt
        FROM sc_type
        GROUP BY type_name, type_pid
        HAVING cnt > 1
        ORDER BY type_pid, type_name
    """)
    
    duplicates = []
    for row in cursor.fetchall():
        keep_id = row['keep_id']
        all_ids = [int(x) for x in row['all_ids'].split(',')]
        delete_ids = [x for x in all_ids if x != keep_id]
        
        for del_id in delete_ids:
            duplicates.append({
                'keep_id': keep_id,
                'delete_id': del_id,
                'name': row['type_name'],
                'parent_id': row['type_pid']
            })
    
    return duplicates


def get_parent_name(conn, parent_id):
    """è·å–çˆ¶åˆ†ç±»åç§°"""
    if parent_id == 0:
        return "(é¡¶çº§åˆ†ç±»)"
    cursor = conn.cursor()
    cursor.execute("SELECT type_name FROM sc_type WHERE type_id = ?", (parent_id,))
    result = cursor.fetchone()
    return result['type_name'] if result else f"ID:{parent_id}"


def merge_categories(dry_run=False):
    """
    åˆå¹¶é‡å¤åˆ†ç±»
    
    Args:
        dry_run: å¦‚æœä¸º Trueï¼Œåªæ˜¾ç¤ºä¼šåšä»€ä¹ˆï¼Œä¸å®é™…æ‰§è¡Œ
    """
    print("=" * 70)
    print("ğŸ”„ åˆå¹¶é‡å¤åˆ†ç±»")
    print("=" * 70)
    
    if dry_run:
        print("\nâš ï¸  é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…ä¿®æ”¹æ•°æ®\n")
    
    # åˆ›å»ºå¤‡ä»½
    if not dry_run:
        backup_name = f"{MAIN_DB}.{datetime.now().strftime('%Y%m%d%H%M%S')}.merge.bak"
        shutil.copy2(MAIN_DB, backup_name)
        print(f"âœ… å·²åˆ›å»ºå¤‡ä»½: {backup_name}")
    
    conn = get_db_connection(MAIN_DB)
    cursor = conn.cursor()
    
    # 1. æŸ¥æ‰¾é‡å¤åˆ†ç±»
    print("\nğŸ“Š æŸ¥æ‰¾é‡å¤åˆ†ç±»...")
    duplicates = find_duplicate_categories(conn)
    
    if not duplicates:
        print("âœ… æ²¡æœ‰å‘ç°é‡å¤åˆ†ç±»ï¼")
        conn.close()
        return
    
    print(f"å‘ç° {len(duplicates)} å¯¹é‡å¤åˆ†ç±»ï¼š\n")
    
    # æŒ‰çˆ¶åˆ†ç±»åˆ†ç»„æ˜¾ç¤º
    current_parent = None
    for dup in duplicates:
        if dup['parent_id'] != current_parent:
            current_parent = dup['parent_id']
            parent_name = get_parent_name(conn, current_parent)
            print(f"\nğŸ“ {parent_name} (pid={current_parent}):")
        
        print(f"   [{dup['keep_id']:2d}] â† [{dup['delete_id']:2d}]  {dup['name']}")
    
    # 2. ç»Ÿè®¡å½±å“
    print("\n" + "=" * 70)
    print("ğŸ“ˆ å½±å“åˆ†æ")
    print("=" * 70)
    
    total_vod_updates = 0
    total_mapping_updates = 0
    
    for dup in duplicates:
        # ç»Ÿè®¡å—å½±å“çš„è§†é¢‘æ•°
        cursor.execute("SELECT COUNT(*) FROM sc_vod WHERE vod_type_id = ?", (dup['delete_id'],))
        vod_count = cursor.fetchone()[0]
        total_vod_updates += vod_count
        
        if vod_count > 0:
            print(f"   [{dup['delete_id']}] {dup['name']}: {vod_count} æ¡è§†é¢‘å°†è¿ç§»åˆ° [{dup['keep_id']}]")
    
    # ç»Ÿè®¡æ˜ å°„æ›´æ–°
    cursor.execute("SELECT res_source_id, res_name, res_mapping FROM sc_config")
    sources = cursor.fetchall()
    
    for source in sources:
        if source['res_mapping']:
            try:
                mapping = json.loads(source['res_mapping'])
                updates_needed = []
                
                for remote_id, local_id in mapping.items():
                    for dup in duplicates:
                        if local_id == dup['delete_id']:
                            updates_needed.append((remote_id, dup['delete_id'], dup['keep_id']))
                
                if updates_needed:
                    print(f"\n   ğŸ“¡ {source['res_name']}:")
                    for remote_id, old_id, new_id in updates_needed:
                        print(f"      è¿œç¨‹{remote_id}: {old_id} â†’ {new_id}")
                    total_mapping_updates += len(updates_needed)
            except:
                pass
    
    print(f"\nğŸ“Š æ€»è®¡:")
    print(f"   - è§†é¢‘æ›´æ–°: {total_vod_updates} æ¡")
    print(f"   - æ˜ å°„æ›´æ–°: {total_mapping_updates} æ¡")
    print(f"   - åˆ†ç±»åˆ é™¤: {len(duplicates)} ä¸ª")
    
    if dry_run:
        print("\nğŸ’¡ è¿è¡Œ 'python3 merge_categories.py' (ä¸å¸¦ --dry-run) æ‰§è¡Œåˆå¹¶")
        conn.close()
        return
    
    # 3. ç¡®è®¤æ‰§è¡Œ
    print("\n" + "=" * 70)
    confirm = input("âš ï¸  ç¡®è®¤æ‰§è¡Œåˆå¹¶ï¼Ÿ(yes/no): ").strip().lower()
    
    if confirm != 'yes':
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        conn.close()
        return
    
    # 4. æ‰§è¡Œåˆå¹¶
    print("\nğŸ”„ æ‰§è¡Œåˆå¹¶...")
    
    vod_updated = 0
    mapping_updated = 0
    types_deleted = 0
    
    for dup in duplicates:
        # æ›´æ–°è§†é¢‘
        cursor.execute(
            "UPDATE sc_vod SET vod_type_id = ? WHERE vod_type_id = ?",
            (dup['keep_id'], dup['delete_id'])
        )
        vod_updated += cursor.rowcount
    
    # æ›´æ–°æ˜ å°„
    for source in sources:
        if source['res_mapping']:
            try:
                mapping = json.loads(source['res_mapping'])
                updated = False
                
                for remote_id in list(mapping.keys()):
                    local_id = mapping[remote_id]
                    for dup in duplicates:
                        if local_id == dup['delete_id']:
                            mapping[remote_id] = dup['keep_id']
                            updated = True
                            mapping_updated += 1
                
                if updated:
                    cursor.execute(
                        "UPDATE sc_config SET res_mapping = ? WHERE res_source_id = ?",
                        (json.dumps(mapping, ensure_ascii=False), source['res_source_id'])
                    )
            except:
                pass
    
    # åˆ é™¤é‡å¤åˆ†ç±»
    for dup in duplicates:
        cursor.execute("DELETE FROM sc_type WHERE type_id = ?", (dup['delete_id'],))
        types_deleted += cursor.rowcount
    
    conn.commit()
    
    print(f"\nâœ… åˆå¹¶å®Œæˆ:")
    print(f"   - è§†é¢‘æ›´æ–°: {vod_updated} æ¡")
    print(f"   - æ˜ å°„æ›´æ–°: {mapping_updated} æ¡")
    print(f"   - åˆ†ç±»åˆ é™¤: {types_deleted} ä¸ª")
    
    # 5. åŒæ­¥åˆ°ä¸´æ—¶æ•°æ®åº“
    print(f"\nğŸ”„ åŒæ­¥åˆ° {TEMP_DB}...")
    conn.close()
    shutil.copy2(MAIN_DB, TEMP_DB)
    print(f"âœ… åŒæ­¥å®Œæˆ")
    
    # 6. éªŒè¯ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“‹ éªŒè¯ç»“æœ")
    print("=" * 70)
    
    conn = get_db_connection(MAIN_DB)
    remaining = find_duplicate_categories(conn)
    
    if remaining:
        print(f"âš ï¸  ä»æœ‰ {len(remaining)} å¯¹é‡å¤åˆ†ç±»")
    else:
        print("âœ… æ²¡æœ‰é‡å¤åˆ†ç±»")
    
    # æ˜¾ç¤ºæœ€ç»ˆåˆ†ç±»ç»“æ„
    cursor = conn.cursor()
    cursor.execute("SELECT type_id, type_name FROM sc_type WHERE type_pid = 0 ORDER BY type_id")
    l1_list = cursor.fetchall()
    
    print("\nğŸ“‚ æœ€ç»ˆåˆ†ç±»ç»“æ„:")
    for l1 in l1_list:
        cursor.execute(
            "SELECT COUNT(*) FROM sc_type WHERE type_pid = ?",
            (l1['type_id'],)
        )
        l2_count = cursor.fetchone()[0]
        print(f"   [{l1['type_id']:2d}] {l1['type_name']} ({l2_count} ä¸ªå­åˆ†ç±»)")
    
    conn.close()
    print("\n" + "=" * 70)
    print("ğŸ‰ å®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='åˆå¹¶é‡å¤åˆ†ç±» - ç›¸åŒåç§°å’Œçˆ¶åˆ†ç±»çš„åˆ†ç±»ä¿ç•™æœ€å°ID'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…æ‰§è¡Œ'
    )
    
    args = parser.parse_args()
    merge_categories(dry_run=args.dry_run)
